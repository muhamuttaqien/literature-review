{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6727be9-71c3-457b-ad15-8cbf6595a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4776dd50-a5c0-4dc5-89d9-e8e3c15d12e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CurriculumEnv(gym.Env):\n",
    "    def __init__(self, difficulty=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.difficulty = difficulty\n",
    "        self.max_steps = 100\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # Left, Right, Up, Down\n",
    "        self.observation_space = spaces.Box(low=0, high=9, shape=(2,), dtype=np.int32)\n",
    "\n",
    "        self.target = np.array([9, 9])\n",
    "        self.agent_position = np.array([0, 0])\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.array([0, 0])\n",
    "        self.step_count = 0\n",
    "        self.update_target_position()\n",
    "        return self.agent_position\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.step_count >= self.max_steps:\n",
    "            return self.agent_position, 0, True, {}\n",
    "\n",
    "        # Update agent's position\n",
    "        move = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # Left, Right, Up, Down\n",
    "        self.agent_position += move[action]\n",
    "        self.agent_position = np.clip(self.agent_position, 0, 9)  # Keep within bounds\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Calculate reward\n",
    "        distance = np.linalg.norm(self.target - self.agent_position)\n",
    "        reward = max(0, 1 - distance / 10)\n",
    "\n",
    "        done = np.array_equal(self.agent_position, self.target)\n",
    "        if done:\n",
    "            reward += 1  # Bonus for reaching the target\n",
    "            self.difficulty += 1\n",
    "            self.update_target_position()\n",
    "\n",
    "        return self.agent_position, reward, done, {}\n",
    "\n",
    "    def update_target_position(self):\n",
    "        positions = [(9, 9), (7, 7), (5, 5)]  # Easy, Medium, Hard positions\n",
    "        self.target = np.array(positions[min(self.difficulty - 1, 2)])\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((10, 10), '.', dtype=str)\n",
    "        grid[self.agent_position[1], self.agent_position[0]] = 'A'\n",
    "        grid[self.target[1], self.target[0]] = 'T'\n",
    "\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print(f\"Difficulty: {self.difficulty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4ffb8f-1ade-46c5-9536-8833a92ecd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CurriculumEnv(difficulty=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b707f-bfe6-41e9-b7a0-6a9c9fb71b22",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9821a046-5708-4d15-b2bc-a9f672356ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of episodes to run\n",
    "num_episodes = 500\n",
    "\n",
    "# Gradual increase in difficulty after every X episodes\n",
    "difficulty_increase_interval = 50  # Increase difficulty every 50 episodes\n",
    "\n",
    "def train():\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()  # Reset the environment at the start of each episode\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose a random action\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # Take the action, observe the next state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Accumulate the reward for this episode\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "\n",
    "        # Gradually increase the difficulty after every few episodes\n",
    "        if episode % difficulty_increase_interval == 0 and env.difficulty < 3:\n",
    "            env.difficulty += 1\n",
    "            env.update_target_position()\n",
    "\n",
    "        # Every 10 episodes, print the total reward and current difficulty\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Total Reward: {total_reward}, Difficulty: {env.difficulty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e877eae7-1c44-4ef4-a77d-f859b1ce0438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward: 52.17904328283545, Difficulty: 3\n",
      "Episode 10: Total Reward: 31.5290751809918, Difficulty: 6\n",
      "Episode 20: Total Reward: 50.95322721335424, Difficulty: 9\n",
      "Episode 30: Total Reward: 55.69061859466779, Difficulty: 12\n",
      "Episode 40: Total Reward: 56.73208885050124, Difficulty: 13\n",
      "Episode 50: Total Reward: 34.49027160341148, Difficulty: 20\n",
      "Episode 60: Total Reward: 58.64947603099142, Difficulty: 23\n",
      "Episode 70: Total Reward: 54.34688466578177, Difficulty: 25\n",
      "Episode 80: Total Reward: 53.26550650414445, Difficulty: 27\n",
      "Episode 90: Total Reward: 46.03125751213005, Difficulty: 31\n",
      "Episode 100: Total Reward: 59.48644178107734, Difficulty: 34\n",
      "Episode 110: Total Reward: 45.35316424907134, Difficulty: 39\n",
      "Episode 120: Total Reward: 61.41921879427215, Difficulty: 42\n",
      "Episode 130: Total Reward: 51.50696262335984, Difficulty: 45\n",
      "Episode 140: Total Reward: 27.60220187415643, Difficulty: 48\n",
      "Episode 150: Total Reward: 52.93046766562184, Difficulty: 49\n",
      "Episode 160: Total Reward: 55.08566443080433, Difficulty: 55\n",
      "Episode 170: Total Reward: 50.85784503291234, Difficulty: 60\n",
      "Episode 180: Total Reward: 44.12078311074518, Difficulty: 62\n",
      "Episode 190: Total Reward: 33.226307963079975, Difficulty: 68\n",
      "Episode 200: Total Reward: 52.578507192020176, Difficulty: 72\n",
      "Episode 210: Total Reward: 46.16429419988637, Difficulty: 75\n",
      "Episode 220: Total Reward: 29.91202249553912, Difficulty: 81\n",
      "Episode 230: Total Reward: 19.644902225243552, Difficulty: 85\n",
      "Episode 240: Total Reward: 52.98246672139784, Difficulty: 89\n",
      "Episode 250: Total Reward: 52.948524823281446, Difficulty: 94\n",
      "Episode 260: Total Reward: 57.14954138697105, Difficulty: 98\n",
      "Episode 270: Total Reward: 48.99710989364382, Difficulty: 100\n",
      "Episode 280: Total Reward: 53.78758806199337, Difficulty: 104\n",
      "Episode 290: Total Reward: 50.259530762820035, Difficulty: 108\n",
      "Episode 300: Total Reward: 56.070078962420816, Difficulty: 110\n",
      "Episode 310: Total Reward: 55.16623478764602, Difficulty: 115\n",
      "Episode 320: Total Reward: 41.99259270894188, Difficulty: 119\n",
      "Episode 330: Total Reward: 12.351598174494674, Difficulty: 123\n",
      "Episode 340: Total Reward: 41.995262825714015, Difficulty: 127\n",
      "Episode 350: Total Reward: 54.10313079937336, Difficulty: 130\n",
      "Episode 360: Total Reward: 41.85947859118376, Difficulty: 133\n",
      "Episode 370: Total Reward: 17.204375709474284, Difficulty: 135\n",
      "Episode 380: Total Reward: 47.30836697078932, Difficulty: 141\n",
      "Episode 390: Total Reward: 55.52753816394707, Difficulty: 145\n",
      "Episode 400: Total Reward: 65.82179058328202, Difficulty: 150\n",
      "Episode 410: Total Reward: 61.64768120146821, Difficulty: 151\n",
      "Episode 420: Total Reward: 56.59864418033634, Difficulty: 155\n",
      "Episode 430: Total Reward: 58.037526073588346, Difficulty: 158\n",
      "Episode 440: Total Reward: 48.93844851426672, Difficulty: 161\n",
      "Episode 450: Total Reward: 53.35964382249206, Difficulty: 164\n",
      "Episode 460: Total Reward: 51.18260275517441, Difficulty: 167\n",
      "Episode 470: Total Reward: 55.05750998601082, Difficulty: 168\n",
      "Episode 480: Total Reward: 23.786574972340375, Difficulty: 172\n",
      "Episode 490: Total Reward: 53.21851511208163, Difficulty: 174\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d15976-72d9-4f18-a2bf-8db5f37e49fa",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muhammad-ra)",
   "language": "python",
   "name": "muhammad-ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
